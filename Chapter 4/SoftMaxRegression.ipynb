{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-10T15:53:35.471561Z",
     "iopub.status.busy": "2025-08-10T15:53:35.471258Z",
     "iopub.status.idle": "2025-08-10T15:53:35.483051Z",
     "shell.execute_reply": "2025-08-10T15:53:35.482013Z",
     "shell.execute_reply.started": "2025-08-10T15:53:35.471537Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 'input\\Iris.csv' already exists. Skipping download and move.\n",
      "\n",
      "Files in the 'input' directory:\n",
      "input\\Iris.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\kenak\\Desktop\\Absolutely new untouched folder\\venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import kagglehub # type: ignore\n",
    "import shutil\n",
    "\n",
    "# Define Paths\n",
    "destination_dir = 'input'\n",
    "# Assuming the dataset will always contain 'Iris.csv'\n",
    "file_name = 'Iris.csv'\n",
    "destination_file = os.path.join(destination_dir, file_name)\n",
    "\n",
    "# Create destination directory\n",
    "os.makedirs(destination_dir, exist_ok=True)\n",
    "\n",
    "# Check if the file already exists\n",
    "if os.path.exists(destination_file):\n",
    "    print(f\"File '{destination_file}' already exists. Skipping download and move.\")\n",
    "else:\n",
    "    # Download and move to input dir\n",
    "    print(\"Downloading dataset...\")\n",
    "    try:\n",
    "        path = kagglehub.dataset_download(\"endofnight17j03/iris-classification\")\n",
    "        print(f\"Dataset downloaded to: {path}\")\n",
    "\n",
    "        source_file = os.path.join(path, file_name)\n",
    "        \n",
    "        if not os.path.exists(source_file):\n",
    "             raise FileNotFoundError(f\"CSV file '{file_name}' not found in downloaded path: {path}\")\n",
    "\n",
    "        print(f\"Moving '{source_file}' to '{destination_file}'...\")\n",
    "        shutil.move(source_file, destination_file)\n",
    "        print(\"Move complete.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {e}\")\n",
    "\n",
    "# Verify the file is in the new directory \n",
    "print(\"\\nFiles in the 'input' directory:\")\n",
    "if os.path.exists(destination_dir):\n",
    "    for dirname, _, filenames in os.walk(destination_dir):\n",
    "        for filename in filenames:\n",
    "            print(os.path.join(dirname, filename))\n",
    "else:\n",
    "    print(\"Input directory does not exist.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Softmax Regression\n",
    "\n",
    "The purporse of this notebook is to create a model using softmax regression "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is softmax regression? \n",
    "\n",
    "Softmax regression id a generalization of logistic regression as to support multiclass prediction natively, thus while logistic regression would give us the probability of being in a class or not in a class for every feature in our target, softmax provides us with a single set of probabilities with our predicted class being the class with the highest probability. Softmax is recommended when the classes that we are trying to predict are mutually exclusive.\n",
    "\n",
    "We would like to implement mini-batch gradient descent to use to train or classifier. This is identical to batch GD the only difference beign that we train on a batch of m instances in each iteration instead of the whole dataset, this is useful when implementing out of core learning and when dealing with very large datasets. We will start off with that before taking a look at our algorithm.\n",
    "\n",
    "As to properly implement, illustrate and take advantage of minibatch learning, we will download our dataset from sklearn instead of just importing it, I guess if it is large enough it is downloaded anyway. Thus we will not directly have it in our RAM and must read from file. The dataset that I will using this on is the iris dataset which is commically small compared to other irl datasets, but the fundamentals still apply, so let us get into it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "import pathlib\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "\n",
    "class MiniBatchGD():\n",
    "    \"\"\"\n",
    "    Implementation of the Batch Gradient Descent algorithm with automatic calling of the \n",
    "    MiniBachSample class for the computation of MiniBatches\n",
    "    \"\"\"\n",
    "    def __init__(self,path:str|pathlib.Path):\n",
    "        self.coef_: np.ndarray\n",
    "        self.n_features_in_: int\n",
    "        self.feature_names_in_: np.ndarray\n",
    "        self.path:str|pathlib.Path\n",
    "\n",
    "    def fit(self, X_index:np.ndarray,y_index:np.ndarray, path: str|pathlib.Path, lines:int, epochs: int = 100, eta:float=0.1, gain:float=0.1):\n",
    "        '''Assuming that all features are numeric, just a fine assumption for the iris dataset,\n",
    "        don't feel like extending it right now, may implement it in a differen way in the future possibly\n",
    "        \n",
    "        Should have used my own standard scaler since itis pure numpy, will save me some time \n",
    "\n",
    "        '''\n",
    "        sampler = MiniBatchSampler()\n",
    "        df = sampler.sample(path,lines)\n",
    "        if df.dtypes.iloc[y_index] == 'object':\n",
    "            categories = df.iloc[:,y_index]\n",
    "            category_map: dict[str,int] = defaultdict(int)\n",
    "            for index, cat in enumerate(categories):\n",
    "                # We assign the first index we see with the value as the ordinal encoding\n",
    "                # We will have to normalize this in the end\n",
    "                if str(cat) in category_map:\n",
    "                    df.iloc[index, y_index] = category_map[str(cat)]\n",
    "                else:\n",
    "                    category_map[str(cat)] = index\n",
    "                    df.iloc[index, y_index] = category_map[str(cat)]\n",
    "        \n",
    "        # Scale features and target\n",
    "        # I will permit myself to use the standard scaler here as I have already implemented it and don't \n",
    "        # want to create a new package, neither do I want to copy the code here\n",
    "\n",
    "        sclr = StandardScaler()\n",
    "\n",
    "        # Ok, so granted I hadn't implemented partial fit yet, \n",
    "        # And I am tired, I'll give myself this\n",
    "        dataset = df.to_numpy()\n",
    "        sclr.partial_fit(dataset)\n",
    "        dataset = sclr.transform(dataset)\n",
    "        X,y = dataset[:,:y_index],df.to_numpy()[:,y_index]\n",
    "        rng = np.random.default_rng()\n",
    "        theta = rng.random(size=(X.shape[1],1))\n",
    "        \n",
    "\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MiniBatchSampler():\n",
    "    \"\"\"\n",
    "    MiniBatchSampler class for mini-batch sampling from large CSV files.\n",
    "\n",
    "    This class enables loading random subsets (mini-batches) of data from a CSV file\n",
    "    without reading the entire file into memory. It uses reservoir sampling for the\n",
    "    initial batch and generates random samples for subsequent batches.\n",
    "\n",
    "    Attributes:\n",
    "        sample_ (pd.DataFrame): The current mini-batch sample.\n",
    "        columns (pd.Index): Column names of the dataset.\n",
    "        DataFrame (pd.DataFrame): DataFrame holding the current batch.\n",
    "        length (int): Total number of data lines in the CSV (excluding header).\n",
    "\n",
    "    Methods:\n",
    "        init_df(data): Initialize or update the DataFrame with new data.\n",
    "        sample(path, lines, header): Sample a mini-batch of lines from the CSV file.\n",
    "    \"\"\"\n",
    "    # The goal of mini-batch is to load a random subset of lines.\n",
    "    # We approach the problem with a slightly modified version of reservoir sampling.\n",
    "    # In the first pass, we create a sample and count the total lines simultaneously.\n",
    "    # In subsequent passes, we use the known line count to efficiently seek to random lines.\n",
    "\n",
    "    def __init__(self):\n",
    "        self.columns: pd.Index|None = None\n",
    "        self.DataFrame: pd.DataFrame = pd.DataFrame()\n",
    "        self.length: int = 0\n",
    "        self.path: str|pathlib.Path = str()\n",
    "        self.header: bool = True\n",
    "        self.lines: int = 0\n",
    "        self.rng:np.random.Generator = np.random.default_rng(42)\n",
    "\n",
    "\n",
    "    def init_df(self, data: list[list[str]]):\n",
    "        '''\n",
    "        Initialize or update the DataFrame with new data.\n",
    "        '''\n",
    "        self.DataFrame = pd.DataFrame(data, columns=self.columns)\n",
    "\n",
    "    def sample(self, path: str|pathlib.Path, lines: int, header: bool = True) -> pd.DataFrame:\n",
    "        '''\n",
    "        This is an alright approach for my dataset, for others just use pd.read_csv\n",
    "        for more robust string parsing.\n",
    "        '''\n",
    "        rng = self.rng\n",
    "        self.path = path\n",
    "        self.header = header\n",
    "        self.lines = lines\n",
    "        first_pass = self.DataFrame.empty or self.length == 0\n",
    "        with open(path, 'r', encoding=\"utf-8\") as f:\n",
    "            # First pass: Use Reservoir Sampling for a true one-pass initial sample.\n",
    "            if first_pass:\n",
    "                if header:\n",
    "                    columns_line = f.readline()\n",
    "                    self.columns = pd.Index(columns_line.strip().split(','))\n",
    "                \n",
    "                reservoir: list[list[str]] = []\n",
    "                line_count = 0\n",
    "                for line in f:\n",
    "                    line_count += 1\n",
    "                    # Fill the reservoir with the first `lines` items\n",
    "                    if len(reservoir) < lines:\n",
    "                        reservoir.append(line.strip().split(','))\n",
    "                    else:\n",
    "                        # For subsequent items, replace an existing item with a decreasing probability\n",
    "                        j = rng.integers(0, line_count)\n",
    "                        if j < lines:\n",
    "                            reservoir[j] = line.strip().split(',')\n",
    "                \n",
    "                self.length = line_count\n",
    "                self.init_df(reservoir)\n",
    "                return self.DataFrame\n",
    "            else:\n",
    "                # We know how many lines we have, so it is more efficient to just generate random numbers\n",
    "                # and go to them (in the best case). We will implement a one-pass sampling to fetch the required lines.\n",
    "\n",
    "                # 1. Generate `lines` unique random indices to fetch from the `self.length` total data lines.\n",
    "                # We sort them to be able to read the file in a single forward pass.\n",
    "                indices_to_read = sorted(rng.choice(self.length, size=lines, replace=False))\n",
    "\n",
    "                new_data: list[list[str]] = []\n",
    "                indices_ptr = 0\n",
    "\n",
    "                # 2. Go to the start of the file and read the lines at the chosen indices.\n",
    "                f.seek(0)\n",
    "                if header:\n",
    "                    f.readline()  # Skip the header row\n",
    "\n",
    "                for i, line in enumerate(f):\n",
    "                    if indices_ptr < len(indices_to_read) and i == indices_to_read[indices_ptr]:\n",
    "                        # This is a line we want, so we parse it and add it to our new data\n",
    "                        line_data = line.strip().split(',')\n",
    "                        new_data.append(line_data)\n",
    "                        indices_ptr += 1\n",
    "                    \n",
    "                    if indices_ptr == len(indices_to_read):\n",
    "                        # If we have found all our lines, we can stop reading the file.\n",
    "                        break\n",
    "                \n",
    "                # 3. Replace the content of the DataFrame with the new batch.\n",
    "                self.init_df(new_data)\n",
    "                return self.DataFrame\n",
    "\n",
    "    def resample(self) -> pd.DataFrame:\n",
    "        if self.length < self.lines:\n",
    "            return self.DataFrame\n",
    "\n",
    "        with open(self.path,'r',encoding='utf-8') as f:\n",
    "            # 1. Generate `lines` unique random indices to fetch from the `self.length` total data lines.\n",
    "            # We sort them to be able to read the file in a single forward pass.\n",
    "            indices_to_read = sorted(self.rng.choice(self.length, size=self.lines, replace=False))\n",
    "            \n",
    "            new_data: list[list[str]] = []\n",
    "            indices_ptr = 0\n",
    "            # 2. Go to the start of the file and read the lines at the chosen indices.\n",
    "            f.seek(0)\n",
    "            if self.header:\n",
    "                f.readline()  # Skip the header row\n",
    "            for i, line in enumerate(f):\n",
    "                if indices_ptr < len(indices_to_read) and i == indices_to_read[indices_ptr]:\n",
    "                    # This is a line we want, so we parse it and add it to our new data\n",
    "                    line_data = line.strip().split(',')\n",
    "                    new_data.append(line_data)\n",
    "                    indices_ptr += 1\n",
    "                \n",
    "                if indices_ptr == len(indices_to_read):\n",
    "                    # If we have found all our lines, we can stop reading the file.\n",
    "                    break\n",
    "            \n",
    "            # 3. Replace the content of the DataFrame with the new batch.\n",
    "            self.init_df(new_data)\n",
    "            return self.DataFrame\n",
    "\n",
    "        \n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 3740158,
     "sourceId": 6474646,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31089,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
