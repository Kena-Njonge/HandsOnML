{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2c6f0c23",
   "metadata": {},
   "source": [
    "# Excursion\n",
    "\n",
    "This is an excourse, not really meant to be read again, just me trying out things while reading the book. It is readable, but will include way more of my thoughts in the code thatn usual.\n",
    "\n",
    "# Excourse on Linear Regression\n",
    "\n",
    "## Normal Equation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17a13a73",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.         0.0773956  0.04388784]\n",
      " [1.         0.08585979 0.0697368 ]\n",
      " [1.         0.00941773 0.09756224]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt # type: ignore\n",
    "from matplotlib.figure import Figure # type: ignore\n",
    "from mpl_toolkits.mplot3d import Axes3D  # type: ignore\n",
    "from sklearn.preprocessing import add_dummy_feature\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Taken from the function x1 + x2=y\n",
    "X = np.array([[1,1],[2,2],[3,3]])\n",
    "y = np.array([[2], [4], [6]])\n",
    "\n",
    "# Intitialise rng \n",
    "rng  = np.random.default_rng(42)\n",
    "\n",
    "# Will throw error because of perfectly correlated data, let us add some random noise beforehand\n",
    "# If you add noise to your bias you get some really whacky stuff with gradient descent\n",
    "X_b_noise = rng.random(size=(3,2))*0.1 \n",
    "X_b = add_dummy_feature(X_b_noise)\n",
    "\n",
    "\n",
    "print(X_b)\n",
    "\n",
    "\n",
    "# So we used y = x1 + x2  to generate the data, let us see how close we get\n",
    "theta_best = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eba36e71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Theta/ Model Parameters[[-1.50836969]\n",
      " [ 1.7874769 ]\n",
      " [76.78724959]]\n",
      "Predictions:[[312.79053628]\n",
      " [391.36526277]]\n",
      "119168.36732462407\n"
     ]
    }
   ],
   "source": [
    "# We would have hoped for one on both, but we basically get an equivalent feature by theta_2 being approx. 2\n",
    "# Error is due to the nois\n",
    "print(f\"Theta/ Model Parameters{theta_best}\")\n",
    "\n",
    "# Make predictions for 2 isntances\n",
    "X_samples_test = np.array([[4,4], [5,5]])\n",
    "y_test = np.array([[8],[10]])\n",
    "\n",
    "X_test = add_dummy_feature(X_samples_test)\n",
    "\n",
    "y_pred = X_test @ theta_best\n",
    "\n",
    "print(f\"Predictions:{y_pred}\")\n",
    "error = mean_squared_error(y_test,y_pred)\n",
    "print(error)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "791b4f1f",
   "metadata": {},
   "source": [
    "We can compute the feature weights (coefficients) and the bias (intercept) in the following way using sklearn and directly using the scipy function that the LinearRegression class is based on. Note that for the calculation of theta we use the pseudoinverse (Moore-Penrose) of X (see Hands on ML). This doesn't seem to be any less accurate, at least for our example, than the closed-form solution (Normal Equation).\n",
    "\n",
    "The pseudoinverse is always defined, which makes it work on singular matirces as well. The calculation of the pseudoinverse uses Singular Value Decomposition (see Hands on ML)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "3ba3264e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction 0: 312.79\n",
      "Prediction 1: 391.37\n",
      "[[312.79053628]\n",
      " [391.36526277]]\n"
     ]
    }
   ],
   "source": [
    "# Doing it with sklearn\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lin_mdl = LinearRegression()\n",
    "\n",
    "lin_mdl.fit(X_b,y)\n",
    "y_pred=lin_mdl.predict(X_test)\n",
    "for i in range(y_pred.shape[0]):\n",
    "\tprint(f\"Prediction {i}: {y_pred[i,0]:.2f}\")\n",
    "print(y_pred)\n",
    "# Equivalent terms\n",
    "bias, weights =  lin_mdl.intercept_,lin_mdl.coef_\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5504cd91",
   "metadata": {},
   "source": [
    "### Implement matrix multiplication, small exercise   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e0bc0ef5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[3. 3. 3.]\n",
      " [3. 3. 3.]\n",
      " [3. 3. 3.]]\n",
      "It failed :(....As expected hahaha <evil> :)\n",
      "[[3. 3.]\n",
      " [3. 3.]\n",
      " [3. 3.]]\n",
      "[[3. 3. 3.]\n",
      " [3. 3. 3.]]\n"
     ]
    }
   ],
   "source": [
    "def matrix_mult(A: np.ndarray,B: np.ndarray) -> np.ndarray:\n",
    "    if not (A.shape[1] == B.shape[0]):\n",
    "        raise ValueError(\"Incompatible Matrix Dimensions:{} {}\".format(A.shape,B.shape) )\n",
    "\n",
    "    C = np.zeros((A.shape[0],B.shape[1]))\n",
    "\n",
    "    for i in range(A.shape[0]):\n",
    "        for j in range(B.shape[1]):\n",
    "            C[i,j] = np.sum(A[i,:]*B[:,j])\n",
    "    \n",
    "    return C\n",
    "\n",
    "A = np.array([[1,1,1],[1,1,1],[1,1,1]])\n",
    "A_test = np.array([[1,1,1],[1,1,1]])\n",
    "B = np.array([[1,1,1],[1,1,1],[1,1,1]])\n",
    "B_false = np.array([[1,1,1],[1,1,1]])\n",
    "B_test = np.array([[1,1],[1,1],[1,1]])\n",
    "\n",
    "print(matrix_mult(A,B))\n",
    "try:\n",
    "    print(matrix_mult(A,B_false))\n",
    "except:\n",
    "    print('It failed :(....As expected hahaha <evil> :)')\n",
    "\n",
    "print(matrix_mult(A,B_test))\n",
    "print(matrix_mult(A_test,B))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19151e11",
   "metadata": {},
   "source": [
    "# Implement gradient descent    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "924a8fae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input Matrix: \n",
      " [[1 1]\n",
      " [2 2]\n",
      " [3 3]]\n",
      "Weights: \n",
      " [[0.93768849]\n",
      " [1.06231151]]\n",
      "Predictions: \n",
      " [[2.]\n",
      " [4.]\n",
      " [6.]]\n",
      "Input Matrix: \n",
      " [[1.         0.0773956  0.04388784]\n",
      " [1.         0.08585979 0.0697368 ]\n",
      " [1.         0.00941773 0.09756224]]\n",
      "Weights: \n",
      " [[ 3.72197331]\n",
      " [-5.55009057]\n",
      " [ 8.48822276]]\n",
      "Predictions: \n",
      " [[3.66495049]\n",
      " [3.8373852 ]\n",
      " [4.49783401]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def gradient_descent(n_epochs:int, X: np.ndarray, y:np.ndarray, eta: float,theta: np.ndarray) -> np.ndarray:\n",
    "    m = X.shape[0]\n",
    "    i = 0\n",
    "    while i<n_epochs:\n",
    "        # gradient vetor, \n",
    "        grad_vec = 2/m * X.T @ (X@theta - y)\n",
    "        theta = theta - eta*grad_vec\n",
    "        i+=1\n",
    "    \n",
    "    return theta\n",
    "\n",
    "for i in [X,X_b]:\n",
    "    theta_rand = rng.random((i.shape[1],1)) *5\n",
    "\n",
    "    eta=0.1\n",
    "    n_epochs=1000\n",
    "    # No need to add dummy parameter when using gradient descent apparently\n",
    "    # Well a dummy parameter (bias) was added by geron, let me add it back\n",
    "    weights = gradient_descent(n_epochs, i,y,eta,theta_rand)\n",
    "\n",
    "    print(f\"Input Matrix: \\n {i}\")\n",
    "    print(f\"Weights: \\n {weights}\")\n",
    "    print(f\"Predictions: \\n {i @ weights}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
