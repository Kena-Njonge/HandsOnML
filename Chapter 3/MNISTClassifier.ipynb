{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":242592,"sourceType":"datasetVersion","datasetId":102285}],"dockerImageVersionId":31089,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import sys\n# Checking version as to correctly type hint, python 3.8 and earlier require imports from the typing module\n# Additionally python 3.9 requires optional and union to be imported\nsys.version\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Fetch the MNIST dataset. The MNIST dataset is a popular ML datasets containing 70000 handwritten digits. It is a popular dataset for people to test new classification methods or ml pipelines on, due to the fact that it is widely used, extensive and contains labels for each set of features. Furthermore the dataset is also good as it is representative of what one may see in real life when analysing handwriting with there being a fair amount of outliers. Although the task might seem simple, the nuance exists in creating a classifier that performs well with a high accuracy, even on outliers.","metadata":{}},{"cell_type":"code","source":"from sklearn.datasets import fetch_openml\nfrom sklearn.neighbors import KNeighborsClassifier\n\nmnist = fetch_openml('mnist_784', as_frame = False)\nX,y = mnist.data, mnist.target \n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let us plot a couple of the images, just to play around with matplotlib and get an idea of what type of data we are working with","metadata":{}},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom PIL import Image\nfrom mpl_toolkits.axes_grid1 import ImageGrid","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%matplotlib inline","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\ndef plot_sample(array: np.array, sample_square_size: int, plot_size: int = 10):\n    if sample_square_size**2 > array.size:\n        raise ValueError(f'Not enough samples in dataset, expected an array of shape: {(sample_square_size**2,784)} or larger')\n    fig = plt.figure(figsize=(plot_size,plot_size))\n    # rect grid starts from top left corner\n    grid = ImageGrid(fig, rect = 111, nrows_ncols=(sample_square_size,sample_square_size), axes_pad=0, label_mode='1',)\n    for index,ax in enumerate(grid):\n        # don't specify extent yet\n        entry = array[index].reshape(28,28)\n        # see https://matplotlib.org/stable/gallery/color/colormap_reference.html for more cmaps\n        ax.imshow(entry, cmap='binary')\n        ax.set_xticks([])\n        ax.set_yticks([])\n\ndef plot_single(array):\n    fig= plt.figure(figsize=(5,5))\n    ax = fig.add_subplot(111)\n    entry = array.reshape(28,28)\n    ax.imshow(entry)\n    ax.set_axis_off()\n\n\n\nplot_sample(X,5)\n    ","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"The most intutitive way to approach such a problem is for each datapoint in our test datset we should classify it based on what image is most similar to it, so instead of learning anything, we lazily evaluate for each new item that we get. This is exactly what the Kneighbors classifier does. ","metadata":{}},{"cell_type":"markdown","source":"Before we proceed, let us isolate our test data. As we are dealign with multiclass data, and do to how the KNClassifier works as we just went over, we want to perform a stratified split. For this dataset, this is very simple as it has already been shulffled and stratified for us.","metadata":{}},{"cell_type":"code","source":"X_train, X_val, X_test, y_train, y_val, y_test = X[:50000], X[50000:60000], X[60000:],y[:50000],y[50000:60000],y[60000:]\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Let us see how our KNNClassifier performs without any hyperparameter tuning","metadata":{}},{"cell_type":"code","source":"knn_clf = KNeighborsClassifier()\nknn_clf.fit(X_train,y_train)\nprediction = knn_clf.predict(X_val[10].reshape(1, -1))\nprint(prediction)\nprint(y_val[10])\nplot_single(X_val[10])\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Great, so for a single entry, even when the 5 looks more like an 's' than a 5 (at this point we are thankful that we are simply dealing with number classification), we can predict which number it is correctly. Let us evaluate the current performance of our classifier on the training set.\n\n\n","metadata":{}},{"cell_type":"code","source":"# Warning, this cell may run for a couple of minutes\n\nfrom sklearn.metrics import f1_score, recall_score, precision_score\nfrom sklearn.model_selection import cross_val_predict\n\n\n\ny_train_pred = cross_val_predict(knn_clf, X_train, y_train, cv=3)\n","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_train, y_train_pred)\nprint(accuracy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"To get to an accuracy above 97%, let us try different parameters for our KNeighboursClassifier","metadata":{}},{"cell_type":"code","source":"\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {'n_neighbors':[3,7,10]}\ngrid_search = GridSearchCV(knn_clf,param_grid, cv=3)\ncv_results = grid_search.fit(X_train, y_train).cv_results_\ncv_df = pd.DataFrame(cv_results)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"As we have now reached a suitable accuracy for our model, thus fulfilling the requiremen","metadata":{}},{"cell_type":"code","source":"cv_df.head()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It seems that our model is struggling to overcome the 96% threshold currently, we thus have 3 options, either decrease the decision threshold or to decrease the amount of neighbors that are taken into consideration or somehow augment our data. Decreasing the amount of neighbors really only seems like a strategy with which we will overfit to this dataset, so now we are left with 2 options. I propose introducing noise to our dataset and this might make the nearest neighbours thus be based on stronger features that persist even after noise is introduced and thus possibly improve our model.","metadata":{}},{"cell_type":"code","source":"print(X_train.shape)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"np.random.seed(42)\nrandom_generator = np.random.default_rng(42)\nrandom_noise = ((255-0) * random_generator.random(size=(50000,784)) + 0)\nrandom_noise = np.round(random_noise,decimals=0)\n\n# Discouraged approach, but simpler\n# random_noise = np.random.randint(0,255,size=(50000,784))\nprint(random_noise.shape)\nX_train_noisy = X_train + random_noise\n\nplot_sample(X,5,5)\nplot_sample(X_train_noisy,5,5)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"knn_clf_noise = KNeighborsClassifier()\nknn_clf_noise.fit(X_train_noisy, y_train)\ny_train_noise_pred = cross_val_predict(knn_clf_noise, X_train_noisy, y_train, cv=3)\n\naccuracy = accuracy_score(y_train, y_train_noise_pred)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(accuracy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Introducing noise decreased our accuracy by 2 points. A note that I made when writing this notebook is that when not usign cross-validation it actually shows an increase of 3 points, thsi jsut shows how important it is to use out of sample data to evaluate the models performance. \n\nI also tested the approach where I introduce noise, but the noise has a maximum value of 125, this was chosen arbitrarily, just to test if introducing noise could somehow improve performance by reducing the weighting of a singular 'feature' of the number for example decreading the weight of the 'cross' in the middle of an 8 by adding noise and thus other features may be weighted higher, this lead to a .2 point decrease in accuracy of the model.\n\nMy next approach that would be inline with is to for each image identify a minimal box around the number and then only introduce noise in that area, thus targeting the positions where we introduce noise. My idea with this is to decrease the weighting of the white pixels. While this idea sounds good in theory it also does not seem effeciently computable to me, I can only think of the bruteforce method in O(n^2) atm just to find the points.\n\nFurthermore, something that I realized is kind of weird about our model is the pixel values, that is, we have black and white images, the colour doesn't matter, but we are taking in the nuances of the color into our model, whereas I as a human when identifying what color would be used, would only be concerned with if I can *clearly* see something there or not. I thus suggest 'smoothing' the images such that every pixel has a binary value of white or black. I realise in hindsight that this is just a form of scaling, albeit an extreme method, so I wonder how this approach performs when compared to min-max standardization or normalization.","metadata":{}},{"cell_type":"code","source":"print(X_train[1])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"X_train_flat = X_train.copy()\n\nfor i in X_train_flat:\n    i[i>0] = 255\n\nplot_sample(X,5,5)\nplot_sample(X_train_flat,5,5)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Just looking at the images I can tell that for some samples the nuance is reduced, but I hope that this will still make us have a higher accuracy and also be generalisable to the validation set. ","metadata":{}},{"cell_type":"code","source":"# I keep on making new classifiers as to compare them in the end.\n# Also knn_clf evaluates lazily, so I do not want to mix up two approaches in one classifier\nknn_clf_flat = KNeighborsClassifier(n_neighbors=5)\nknn_clf_flat.fit(X_train_flat, y_train)\ny_flat_pred = cross_val_predict(knn_clf_flat, X_train_flat, y_train, cv=3)\naccuracy = accuracy_score(y_train, y_flat_pred)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(accuracy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Sadly this approach does not increase the accuracy either, let us scale our dataset and see the effect. Scaling is recommended as ML algorithms perform better when all features have rougly the same scale. Here we have 784 features, so scaling should be quite important. We wil use standardization here due to the fact that most values are either close to 255 or close to 0 meaning if we use a min max scaler, most values would be pushed to 1 resp. 0 with fewer values in the middle. Furthermore, as some of our features have larger scale than others, those features would thus dominate our distance calculation. \n\n","metadata":{}},{"cell_type":"code","source":"from sklearn.preprocessing import StandardScaler, MinMaxScaler","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"stand_scl = StandardScaler()\nX_train_stand =stand_scl.fit_transform(X_train)\n\nmin_max_scl = MinMaxScaler()\nX_train_min_max = min_max_scl.fit_transform(X_train)\n\nknn_std = KNeighborsClassifier()\nknn_min_max  = KNeighborsClassifier()\nknn_std.fit(X_train_stand,y_train)\nknn_min_max.fit(X_train_min_max,y_train)\n\ny_stand_pred = cross_val_predict(knn_std, X_train_stand, y_train, cv=3)\ny_min_max_pred = cross_val_predict(knn_min_max, X_train_min_max, y_train, cv=3)\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"accuracy = accuracy_score(y_train, y_stand_pred)\nprint(accuracy)\naccuracy = accuracy_score(y_train, y_min_max_pred)\nprint(accuracy)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"It seems that through (simple) data augmentation alone we cannot improve the accuracy of the model. Let us one check if any of the models have any merit usign the validation set, we will also plot some precision recall-curves and go into the theory of that for the fun of it,and we will finally move on to finding ideal hyperparameters for our model.","metadata":{}},{"cell_type":"code","source":"# Predictions for flat-trained KNN, without flattened data\ny_flat_val_pred = knn_clf_flat.predict(X_val)\naccuracy = accuracy_score(y_val, y_flat_val_pred)\nprint(f'Flat Accuracy, base data: {accuracy:.4f}')\n\n# Predictions for flat-trained KNN, with flattened data\nX_val_flat = X_val.copy()\nfor i in X_val_flat:\n    i[i>0] = 255\ny_flat_val_pred = knn_clf_flat.predict(X_val_flat)\naccuracy = accuracy_score(y_val, y_flat_val_pred)\nprint(f'Flat Accuracy, flattened data {accuracy:.4f}')\n\n# Predictions for noise-trained KNN without noise addition on dataset\ny_noise_val_pred = knn_clf_noise.predict(X_val)\naccuracy = accuracy_score(y_val, y_noise_val_pred)\nprint(f'Noise Accuracy, base data: {accuracy:.4f}')\n\n# Predictions for noise-trained KNN with noise addition on dataset \nX_val_noise = X_val + random_noise[X_val.shape[0],:]\ny_noise_val_pred = knn_clf_noise.predict(X_val_noise)\naccuracy = accuracy_score(y_val, y_noise_val_pred)\nprint(f'Noise Accuracy, noise transformed: {accuracy:.4f}')\n\n# Predictions for base KNN\ny_val_pred = knn_clf.predict(X_val)\naccuracy = accuracy_score(y_val, y_val_pred)\nprint(f'Base Accuracy: {accuracy:.4f}')\n\n# StandardScaler transformed\nX_val_std = stand_scl.transform(X_val)\ny_std_val_pred = knn_std.predict(X_val_std)\naccuracy = accuracy_score(y_val, y_std_val_pred)\nprint(f'Standard Scaled Accuracy: {accuracy:.4f}')\n\n# MinMaxScaler transformed\nX_val_scl = min_max_scl.transform(X_val)\ny_scl_val_pred = knn_min_max.predict(X_val_scl)\naccuracy = accuracy_score(y_val, y_scl_val_pred)\nprint(f'Min-Max Scaled Accuracy: {accuracy:.4f}')\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can see that the classifiers for flat and noisy data perform worse (or horribly) if the data is not transformed beforehand. Our base classifier has a higher accuracy than on the train set which shows that it is generalising well to unseen data. We can see that the base KNN still performs the best. Let us visualise which classes (numbers) the classifiers perform well on, and which ones they struggle with. We will do this using the PR-curve","metadata":{}},{"cell_type":"code","source":"def plot_pr_curves(y_train_bin, y_score, suptitle:str | None):\n    '''Plot precision/recall curves'''\n    fig, ax = plt.subplots(figsize=(5, 5))  \n\n    for i, label in enumerate(labels):\n        precision, recall, thresholds = precision_recall_curve(y_train_bin[:, i], y_score[:, i])\n        ax.plot(recall, precision, linewidth=2.0, label=f'Class {label}')\n    if suptitle:\n        fig.suptitle(suptitle)\n    ax.set_xlabel('Recall')\n    ax.set_ylabel('Precision')\n    ax.set_title('Precision-Recall Curves')\n    ax.legend(loc='best')\n\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from sklearn.metrics import precision_recall_curve\nfrom sklearn.preprocessing import label_binarize\n\n\nlabels = knn_clf.classes_\n# binarize the labels as to simulate one v rest \ny_train_bin = label_binarize(y_train, classes=labels)\nprint(y_train_bin)\n\nknn_models = [\n    (\"Base KNN\", knn_clf, X_train),\n    (\"Flat KNN\", knn_clf_flat, X_train_flat),\n    (\"Noise KNN\", knn_clf_noise, X_train_noisy),\n    (\"Standard Scaled KNN\", knn_std, X_train_stand),\n    (\"Min-Max Scaled KNN\", knn_min_max, X_train_min_max)\n]\n\nfor title, model, X in knn_models:\n    y_score = cross_val_predict(model, X, y_train, cv=3, method='predict_proba')\n    plot_pr_curves(y_train_bin, y_score, title)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"markdown","source":"Let us go over the code and what the goal of it is. First, we must binarize the classes, this measn transforming it into an array where the position representing the ith class has the value 1 if the sample is part of that class. We do this, as a precision recall curve and the terms precision and recall in general require a positive and negative class. When you call the precision_score or recall_score functions on a multinomial classification prediction and sample it performs some aggregation as to give you one value, but this isn't a native characteristic of said function. Thus for each input binarizing the target gives us an array such that we can treat the targets as  if we have a one v rest classification. \n\nTo draw the precision recall curve, we also need access to the underlying probabilities that were used in the prediciton, for the knn this is simply the relative frequency of the classes among the closest neighbors. By comparing these to the theshold - noting that we have reframed the classification to one v rest - we can determine if at probablity x if we would classify it into the positive or negative class.\n\nAs the probability (threshold) goes higher, we classify more items into the negative class, thus the precision, which is tp/(tp+fp) goes higher and our recall tp/(tp+fn) becomes lower. Conversely if we lower the threshold then our recall will go higher as we will classify even more in the posititve class and thus we will have less fn and more tp. \n\nAs Geron says, the answer to the question 'We want 99% precision' should always be 'At what recall?'. As the graphs show, you can get closer to 100% precision with even thsi model, but at the cost of the recall. \n\nSo, how can we interpret this and what is the use-case of shifting the threshold? Well, there can be use-cases where shifting the threshold is important. Take the current task, a similar system is used in address identification for shipping. Here there are (financial) consequences for misidentifying a '1'  or a '9', it may thus be ideal to sacrifice recall as to have a system where if the number is clearly visible, then it will be identified, but if it isn't we do not risk misidentifying the number. In such cases we can then also trigger further steps in the pipeline for the n-most-likely candidates as to identify one among them, utilising steps that may be computationaly expensive and thus are not implemented for the entire dataset. \n\n\nOverall, many considerations must be made when increasing thresholds, doing so can lead to misleading metrics.\n\nNow let us evaluate the graphs:\n\nWe can see that for our base KNN the easiest class to predict is the class 0 and the most dificult class to predict is class 9 (based on AUC). Overall we can see that we achieve very high precision and recall. \n\nFlattening, introducing noise, and scaling do not significantly improve model performance on easily classifiable digits. In fact, these transformations can slightly reduce the model’s ability to identify such digits with high confidence.\n\nThis can be observed in the precision-recall curves: the trade-off between precision and recall begins earlier (i.e., at lower recall values) for the transformed data.\n\nWhile precision-recall curves are useful for visualizing performance, they are not directly comparable across models, since the actual score thresholds that generate these curves differ between models.\n\nTherefore, we avoid interpreting exact positions or claiming threshold-specific behavior (e.g., “left = high threshold”) across models. Instead, we focus on overall curve shape and area under the curve as rough indicators of classifier confidence and tradeoff behavior.\n\nTo empirically compare two recall-precision curves, we can use the auc() or the average precision score, we will use the latter as it is less forgiving, less optimistic as it is not interpolated \n( see [sklearn docs](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.average_precision_score.html#sklearn.metrics.average_precision_score)).\n\nLet us take the class 0 as a representative of the 'easy to predict' class and the class 9 as a representative of the 'hard to predict' class and see how our 'flattened' model compares to our base model. \n","metadata":{}},{"cell_type":"code","source":"from sklearn.metrics import average_precision_score\n\ny_val_bin = label_binarize(y_val,classes=labels)\n\n# Out of sample predictions to produce metrics for the model\ny_score_flat = knn_clf_flat.predict_proba(X_val)\ny_score_base = knn_clf.predict_proba(X_val)\n\n\n# Provide scores for both curves for both models\naps_0 =average_precision_score(y_val_bin[:,0], y_score_flat[:,0])\nprint(f'Average Precision Score on Label 0 for Model \"Flat\": {aps_0}')\naps_0 = average_precision_score(y_val_bin[:,0], y_score_base[:,0])\nprint(f'Average Precision Score on Label 0 for Model \"Base\": {aps_0}')\n\naps_9 =average_precision_score(y_val_bin[:,9], y_score_flat[:,9])\nprint(f'Average Precision Score on Label 9 for Model \"Flat\": {aps_9}')\naps_9 = average_precision_score(y_val_bin[:,9], y_score_base[:,9])\nprint(f'Average Precision Score on Label 9 for Model \"Base\": {aps_9}')\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"We can see here empirically that the base model performs considerably better on difficult numbers and slightly better on easy numbers.\n\nIn our previous grid search, we did not take into account all the hyperparameters that can be used in the KNN, let us update our grid search to find ideal hyperparameters. We expect to see considerably different performance when different (minkowski) distance parameters are used. A general rule of thumb is that the higher the p value, the higher weighted the largest value will be, with p -> inf giving us the max distance along one dimension.\n\nAdditionally, we can also change the parameter 'weight', with which closer neighbors will have a greater influence than neighbors further away.","metadata":{}},{"cell_type":"markdown","source":"**Note**: maybe don't use so many parameters next time, also you had already checked and seen that n_neighbors barely does something, so you could've left that out. ","metadata":{}},{"cell_type":"code","source":"from sklearn.model_selection import GridSearchCV\n\n\n# Warning!!!!! this cell will run for a significant amount of time\n# Will train 24 models \nparam_grid = {'weights':['uniform','distance'],'p':[1,2]}\ngrid_search = GridSearchCV(knn_clf,param_grid, cv=3)\ncv_results = grid_search.fit(X_train, y_train).cv_results_\ncv_df = pd.DataFrame(cv_results)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"print(cv_df)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Further data transformations\n\nLet us explore other means to augment our data. As to make our model less reliant on the position of certain pixels and instead to generalise away from that, we can shift the image in the x and y axes (as recommended by Geron) and add this to our dataset. This is a relatively expensive transformation as far as I can tell, running in O(n^2).","metadata":{}},{"cell_type":"code","source":"def shift_image(image: np.array,x_direction:None, y_direction:None):\n    image = image.reshape(28,28)\n    if x_direction:\n        image_dup = image.copy()\n        for row in range(0,28):\n            image_dup[row] = image[(row+x_direction) % 28]\n        image = image_dup\n    if y_direction:\n        image_dup = image.copy()\n        for col in range(0,28):\n            image_dup = image.copy()\n            image_dup[:,row] = image[(row+y_direction) % 28] \n        image = image_dup\n    return image.reshape(,784)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Higher accuracy, eureka! But does this behaviour generise to the train set without modifications or the validation set?","metadata":{}}]}